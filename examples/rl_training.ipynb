{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38fce3e5",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Training Example\n",
    "\n",
    "This notebook demonstrates how to set up and train an RL agent for Bitcoin futures trading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134e722b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import gymnasium as gym\n",
    "from don.rl.env import TradingEnvironment\n",
    "from don.rl.actions import DiscreteActionSpace\n",
    "from don.rl.rewards import SharpeReward\n",
    "from don.data.binance import BinanceDataCollector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425f7ded",
   "metadata": {},
   "source": [
    "## Load Training Data\n",
    "\n",
    "First, we'll load historical data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e66f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "collector = BinanceDataCollector(\n",
    "    symbol='BTCUSDT',\n",
    "    api_key='your_api_key',\n",
    "    api_secret='your_api_secret'\n",
    ")\n",
    "\n",
    "# Get three months of hourly data\n",
    "end_time = pd.Timestamp.now()\n",
    "start_time = end_time - pd.Timedelta(days=90)\n",
    "training_data = collector.get_historical_data(\n",
    "    start_time=start_time,\n",
    "    end_time=end_time,\n",
    "    interval='1h'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4da1133",
   "metadata": {},
   "source": [
    "## Set Up Trading Environment\n",
    "\n",
    "Create the trading environment with discrete actions and Sharpe ratio reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eeb4640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define action space\n",
    "action_space = DiscreteActionSpace([-1.0, -0.5, 0.0, 0.5, 1.0])\n",
    "\n",
    "# Create environment\n",
    "env = TradingEnvironment(\n",
    "    data=training_data,\n",
    "    action_space=action_space,\n",
    "    reward_calculator=SharpeReward(window=20),\n",
    "    window_size=10\n",
    ")\n",
    "\n",
    "# Test environment\n",
    "observation, info = env.reset()\n",
    "print(\"Observation shape:\", observation.shape)\n",
    "print(\"Action space:\", env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961c62d3",
   "metadata": {},
   "source": [
    "## Implement Simple DQN Agent\n",
    "\n",
    "Create a basic DQN agent for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953b7e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.network = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(32, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Initialize DQN\n",
    "input_dim = env.observation_space.shape[0] * env.observation_space.shape[1]\n",
    "output_dim = len(action_space.positions)\n",
    "dqn = DQN(input_dim, output_dim)\n",
    "optimizer = torch.optim.Adam(dqn.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7dca85",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "Train the agent for a few episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad79a02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_episode():\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        # Convert observation to tensor\n",
    "        obs_tensor = torch.FloatTensor(obs.flatten()).unsqueeze(0)\n",
    "\n",
    "        # Get action from network\n",
    "        with torch.no_grad():\n",
    "            q_values = dqn(obs_tensor)\n",
    "            action = q_values.argmax().item()\n",
    "\n",
    "        # Take action in environment\n",
    "        next_obs, reward, done, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        # Move to next observation\n",
    "        obs = next_obs\n",
    "\n",
    "    return total_reward\n",
    "\n",
    "# Train for 10 episodes\n",
    "for episode in range(10):\n",
    "    reward = train_episode()\n",
    "    print(f\"Episode {episode + 1}, Total Reward: {reward:.2f}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
